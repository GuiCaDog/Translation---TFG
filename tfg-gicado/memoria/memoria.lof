\babel@toc {english}{}\relax 
\babel@toc {catalan}{}\relax 
\babel@toc {english}{}\relax 
\babel@toc {spanish}{}\relax 
\babel@toc {english}{}\relax 
\babel@toc {english}{}\relax 
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Illustration of 4 data-samples representing the XOR problem. Adapted from Figure 13.1 of \cite {pml1Book}.\relax }}{4}{figure.caption.6}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces (a) Multilayer perceptron solving the XOR problem. (b) Representation of possible output values after the first layer ($\phi $). Adapted from Figure 13.1 of \cite {pml1Book}. \relax }}{6}{figure.caption.8}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {\centering }}}{6}{subfigure.2.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {\centering }}}{6}{subfigure.2.2}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Sequence to sequence recurrent neural network with $n$ hidden states and $L$ hidden layers. Adapted from Figure 15.5 of \cite {pml1Book}.\relax }}{10}{figure.caption.11}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces A structural representation of the Transformer model with a certain number of encoding and decoding units.\relax }}{13}{figure.caption.14}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces (a) Representation of a encoding unit from Transformer. (b) Representation of a decoding unit from Transformer where $K$ and $V$ values are obtained from the top-most encoding unit. \relax }}{13}{figure.caption.15}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {\centering }}}{13}{subfigure.5.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {\centering }}}{13}{subfigure.5.2}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Offline systems pipelines and versions.\relax }}{30}{figure.caption.29}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Results on CERN News development set for V4 finetuned on CDS backtranslations parallel set and on CERN News training set.\relax }}{33}{figure.caption.32}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces (a) WMT13 Development set evaluation for different wait-k policies during inference time. (b) Same evaluation on CERN News 21 Development set. \relax }}{39}{figure.caption.34}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {\centering }}}{39}{subfigure.1.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {\centering }}}{39}{subfigure.1.2}%
\addvspace {10\p@ }
